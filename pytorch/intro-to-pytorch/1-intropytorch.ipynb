{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# First, import PyTorch\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def activation(x):\r\n",
    "    \"\"\" Sigmoid activation function \r\n",
    "    \r\n",
    "        Arguments\r\n",
    "        ---------\r\n",
    "        x: torch.Tensor\r\n",
    "    \"\"\"\r\n",
    "    return 1/(1+torch.exp(-x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor in PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "### Generate some data\r\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\r\n",
    "\r\n",
    "# Features are 5 random normal variables\r\n",
    "features = torch.randn((1, 5))\r\n",
    "# True weights for our data, random normal variables again\r\n",
    "weights = torch.randn_like(features)\r\n",
    "# and a true bias term\r\n",
    "bias = torch.randn((1, 1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Now, make our labels from our data and true weights\r\n",
    "\r\n",
    "y = activation(torch.sum(features * weights) + bias)\r\n",
    "y = activation((features * weights).sum() + bias)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# using matrix multiplication\r\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### Generate some data\r\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\r\n",
    "\r\n",
    "# Features are 3 random normal variables\r\n",
    "features = torch.randn((1, 3))\r\n",
    "\r\n",
    "# Define the size of each layer in our network\r\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\r\n",
    "n_hidden = 2                    # Number of hidden units \r\n",
    "n_output = 1                    # Number of output units\r\n",
    "\r\n",
    "# Weights for inputs to hidden layer\r\n",
    "W1 = torch.randn(n_input, n_hidden)\r\n",
    "# Weights for hidden layer to output layer\r\n",
    "W2 = torch.randn(n_hidden, n_output)\r\n",
    "\r\n",
    "# and bias terms for hidden and output layers\r\n",
    "B1 = torch.randn((1, n_hidden))\r\n",
    "B2 = torch.randn((1, n_output))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\r\n",
    "output = activation(torch.mm(h, W2) + B2)\r\n",
    "print(output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks in PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from torchvision import datasets, transforms\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# Define a transform to normalize the data\r\n",
    "transform = transforms.Compose([transforms.ToTensor(),\r\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\r\n",
    "                              ])\r\n",
    "# Download and load the training data\r\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\r\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import helper"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dataiter = iter(trainloader)\r\n",
    "images, labels = dataiter.next()\r\n",
    "print(type(images))\r\n",
    "print(images.shape)\r\n",
    "print(labels.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def activation(x):\r\n",
    "    return 1/(1+torch.exp(-x))\r\n",
    "\r\n",
    "# Flatten the input images\r\n",
    "inputs = images.view(images.shape[0], -1)\r\n",
    "\r\n",
    "# Create parameters\r\n",
    "w1 = torch.randn(784, 256)\r\n",
    "b1 = torch.randn(256)\r\n",
    "\r\n",
    "w2 = torch.randn(256, 10)\r\n",
    "b2 = torch.randn(10)\r\n",
    "\r\n",
    "h = activation(torch.mm(inputs, w1) + b1)\r\n",
    "\r\n",
    "out = activation(torch.mm(h, w2) + b2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def softmax(x):\r\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\r\n",
    "\r\n",
    "probabilities = softmax(out)\r\n",
    "\r\n",
    "# Does it have the right shape? Should be (64, 10)\r\n",
    "print(probabilities.shape)\r\n",
    "# Does it sum to 1?\r\n",
    "print(probabilities.sum(dim=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building neural networks in PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from torch import nn\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class Network(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # Defining the layers, 128, 64, 10 units each\r\n",
    "        self.fc1 = nn.Linear(784, 128)\r\n",
    "        self.fc2 = nn.Linear(128, 64)\r\n",
    "        # Output layer, 10 units - one for each digit\r\n",
    "        self.fc3 = nn.Linear(64, 10)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        ''' Forward pass through the network, returns the output logits '''\r\n",
    "        \r\n",
    "        x = self.fc1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.fc3(x)\r\n",
    "        x = F.softmax(x, dim=1)\r\n",
    "        \r\n",
    "        return x\r\n",
    "\r\n",
    "model = Network()\r\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Create the network and look at its text representation\r\n",
    "model = Network()\r\n",
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# weights and biases\r\n",
    "print(model.fc1.weight)\r\n",
    "print(model.fc1.bias)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0175, -0.0130, -0.0109,  ..., -0.0229, -0.0169, -0.0241],\n",
      "        [-0.0329,  0.0220, -0.0169,  ..., -0.0056, -0.0001, -0.0216],\n",
      "        [ 0.0148, -0.0343,  0.0188,  ..., -0.0123, -0.0170,  0.0136],\n",
      "        ...,\n",
      "        [-0.0169, -0.0015, -0.0003,  ...,  0.0301, -0.0024, -0.0164],\n",
      "        [ 0.0355, -0.0039,  0.0002,  ..., -0.0246,  0.0202,  0.0261],\n",
      "        [ 0.0089, -0.0092, -0.0307,  ..., -0.0253,  0.0020,  0.0346]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0017, -0.0049, -0.0120, -0.0029,  0.0129, -0.0212, -0.0095, -0.0272,\n",
      "         0.0167, -0.0153, -0.0331, -0.0252, -0.0264,  0.0132, -0.0282, -0.0336,\n",
      "         0.0229, -0.0353,  0.0141,  0.0355, -0.0191, -0.0251, -0.0222,  0.0175,\n",
      "        -0.0248, -0.0323,  0.0125,  0.0077,  0.0328, -0.0081, -0.0242, -0.0152,\n",
      "         0.0103, -0.0129, -0.0142, -0.0087,  0.0133, -0.0122, -0.0205,  0.0128,\n",
      "        -0.0118, -0.0239, -0.0298,  0.0182,  0.0261, -0.0310, -0.0199,  0.0118,\n",
      "         0.0146, -0.0155,  0.0077, -0.0020,  0.0151, -0.0246, -0.0023,  0.0027,\n",
      "        -0.0323,  0.0112, -0.0108, -0.0102, -0.0150, -0.0339, -0.0185, -0.0356,\n",
      "         0.0046, -0.0060,  0.0283,  0.0214, -0.0351, -0.0197, -0.0030,  0.0123,\n",
      "        -0.0060,  0.0074, -0.0194, -0.0139,  0.0279, -0.0138,  0.0064,  0.0345,\n",
      "        -0.0011, -0.0270,  0.0175,  0.0341, -0.0322, -0.0281,  0.0223,  0.0166,\n",
      "         0.0139, -0.0187,  0.0214, -0.0171,  0.0324, -0.0039,  0.0077, -0.0136,\n",
      "         0.0333,  0.0136, -0.0237, -0.0184, -0.0162, -0.0214,  0.0255,  0.0212,\n",
      "         0.0129,  0.0132, -0.0072,  0.0303, -0.0292,  0.0258, -0.0063,  0.0204,\n",
      "        -0.0026, -0.0308,  0.0087,  0.0221, -0.0121,  0.0056, -0.0276,  0.0258,\n",
      "        -0.0247, -0.0026,  0.0189, -0.0209, -0.0027,  0.0044, -0.0006,  0.0327],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Neural Networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "# Get our data\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "# Flatten images\r\n",
    "images = images.view(images.shape[0], -1)\r\n",
    "\r\n",
    "# Forward pass, get our logits\r\n",
    "logits = model(images)\r\n",
    "# Calculate the loss with the logits and the labels\r\n",
    "loss = criterion(logits, labels)\r\n",
    "\r\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.3016, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Build a feed-forward network\r\n",
    "model = nn.Sequential(nn.Linear(784, 128),\r\n",
    "                      nn.ReLU(),\r\n",
    "                      nn.Linear(128, 64),\r\n",
    "                      nn.ReLU(),\r\n",
    "                      nn.Linear(64, 10),\r\n",
    "                      nn.LogSoftmax(dim=1))\r\n",
    "\r\n",
    "criterion = nn.NLLLoss()\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images = images.view(images.shape[0], -1)\r\n",
    "\r\n",
    "logps = model(images)\r\n",
    "loss = criterion(logps, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.3167, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\r\n",
    "\r\n",
    "loss.backward()\r\n",
    "\r\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0058,  0.0058,  0.0058,  ...,  0.0058,  0.0058,  0.0058],\n",
      "        [-0.0018, -0.0018, -0.0018,  ..., -0.0018, -0.0018, -0.0018],\n",
      "        ...,\n",
      "        [-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0017,  0.0017,  0.0017],\n",
      "        [ 0.0084,  0.0084,  0.0084,  ...,  0.0084,  0.0084,  0.0084]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from torch import optim\r\n",
    "\r\n",
    "# Optimizers require the parameters to optimize and a learning rate\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "print('Initial weights - ', model[0].weight)\r\n",
    "\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "images.resize_(64, 784)\r\n",
    "\r\n",
    "# Clear the gradients, do this because gradients are accumulated\r\n",
    "optimizer.zero_grad()\r\n",
    "\r\n",
    "# Forward pass, then backward pass, then update weights\r\n",
    "output = model(images)\r\n",
    "loss = criterion(output, labels)\r\n",
    "loss.backward()\r\n",
    "print('Gradient -', model[0].weight.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0075, -0.0140,  0.0248,  ...,  0.0254, -0.0299,  0.0256],\n",
      "        [-0.0188, -0.0249,  0.0065,  ..., -0.0284, -0.0170, -0.0102],\n",
      "        [ 0.0033, -0.0089,  0.0341,  ..., -0.0239, -0.0265, -0.0099],\n",
      "        ...,\n",
      "        [-0.0284,  0.0292, -0.0146,  ..., -0.0259, -0.0287,  0.0313],\n",
      "        [ 0.0277, -0.0311,  0.0033,  ..., -0.0342, -0.0209,  0.0072],\n",
      "        [-0.0170,  0.0180, -0.0288,  ...,  0.0183,  0.0076, -0.0131]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0021,  0.0021,  0.0021],\n",
      "        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        ...,\n",
      "        [-0.0002, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0002],\n",
      "        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n",
      "        [ 0.0034,  0.0034,  0.0034,  ...,  0.0034,  0.0034,  0.0034]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Take an update step and few the new weights\r\n",
    "optimizer.step()\r\n",
    "print('Updated weights - ', model[0].weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0075, -0.0140,  0.0248,  ...,  0.0254, -0.0299,  0.0256],\n",
      "        [-0.0188, -0.0249,  0.0065,  ..., -0.0284, -0.0170, -0.0102],\n",
      "        [ 0.0033, -0.0089,  0.0341,  ..., -0.0239, -0.0264, -0.0099],\n",
      "        ...,\n",
      "        [-0.0284,  0.0292, -0.0146,  ..., -0.0259, -0.0287,  0.0313],\n",
      "        [ 0.0277, -0.0311,  0.0033,  ..., -0.0342, -0.0209,  0.0072],\n",
      "        [-0.0171,  0.0179, -0.0289,  ...,  0.0183,  0.0076, -0.0131]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\r\n",
    "                      nn.ReLU(),\r\n",
    "                      nn.Linear(128, 64),\r\n",
    "                      nn.ReLU(),\r\n",
    "                      nn.Linear(64, 10),\r\n",
    "                      nn.LogSoftmax(dim=1))\r\n",
    "\r\n",
    "criterion = nn.NLLLoss()\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\r\n",
    "\r\n",
    "epochs = 5\r\n",
    "for e in range(epochs):\r\n",
    "    running_loss = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        # Flatten MNIST images into a 784 long vector\r\n",
    "        images = images.view(images.shape[0], -1)\r\n",
    "    \r\n",
    "        # TODO: Training pass\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        output = model(images)\r\n",
    "        loss = criterion(output, labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss += loss.item()\r\n",
    "    else:\r\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss: 1.9706850933876119\n",
      "Training loss: 0.8889987017871983\n",
      "Training loss: 0.5251630273010177\n",
      "Training loss: 0.43221675113701363\n",
      "Training loss: 0.3885586821416548\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import helper\r\n",
    "\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "\r\n",
    "img = images[0].view(1, 784)\r\n",
    "# Turn off gradients to speed up this part\r\n",
    "with torch.no_grad():\r\n",
    "    logps = model(img)\r\n",
    "\r\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\r\n",
    "ps = torch.exp(logps)\r\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "56eb87fbc954af64301db39dd2250c36693ef9dfda1761c1c472f812d1bbbb95"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}